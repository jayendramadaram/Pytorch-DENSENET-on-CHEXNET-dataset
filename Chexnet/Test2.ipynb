{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "import shutil\n",
    "import tarfile\n",
    "import speedtest\n",
    "import torchvision\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "from PIL import Image\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as func\n",
    "import torch.nn.functional as tfunc\n",
    "from IPython.display import display\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "from sklearn.metrics._ranking import roc_auc_score\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DATASET(Dataset):\n",
    "    def __init__(self , transform=None , Train=False ) -> None:\n",
    "        \n",
    "        self.records = []\n",
    "        self.transform = transform\n",
    "        \n",
    "        if not Train:\n",
    "            path = \"dataset/test.txt\"\n",
    "        else:\n",
    "            path = \"TO BE DEFINED\"\n",
    "        \n",
    "        with open(path , \"r\") as file:\n",
    "            # print(len(file.readlines()))\n",
    "            for i in file.readlines():\n",
    "                record = i.split()\n",
    "                item = (record[0] , list(map(int , record[1:])))\n",
    "                self.records.append(item)\n",
    "                # print(os.path.exists(f\"database/{record[0]}\"))\n",
    "                \n",
    "                \n",
    "        \n",
    "        print(\"Total number of Imgs\" , len(self.records))\n",
    "    \n",
    "    def __getitem__(self, index) :\n",
    "        \n",
    "        IMG = self.records[index]\n",
    "        # print(IMG)\n",
    "        PIL_img = Image.open(\"database/\" +IMG[0]).convert(\"RGB\")\n",
    "        LABEL_img = torch.FloatTensor( IMG[1])\n",
    "        if self.transform != None: PIL_img = self.transform(PIL_img)\n",
    "        \n",
    "        return PIL_img , LABEL_img\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.records)\n",
    "        \n",
    "        \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "CKPT_PATH = 'models/model.pth.tar'\n",
    "N_CLASSES = 14\n",
    "CLASS_NAMES = [ 'Atelectasis', 'Cardiomegaly', 'Effusion', 'Infiltration', 'Mass', 'Nodule', 'Pneumonia',\n",
    "                'Pneumothorax', 'Consolidation', 'Edema', 'Emphysema', 'Fibrosis', 'Pleural_Thickening', 'Hernia']\n",
    "DATA_DIR = './ChestX-ray14/images'\n",
    "TEST_IMAGE_LIST = './ChestX-ray14/labels/test_list.txt'\n",
    "BATCH_SIZE = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DenseNet121(nn.Module):\n",
    "    def __init__(self, out_size):\n",
    "        super(DenseNet121, self).__init__()\n",
    "        self.densenet121 = torchvision.models.densenet121(pretrained=True)\n",
    "        num_ftrs = self.densenet121.classifier.in_features\n",
    "        self.densenet121.classifier = nn.Sequential(\n",
    "            nn.Linear(num_ftrs, out_size),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.densenet121(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_AUCs(gt, pred):\n",
    "    \"\"\"Computes Area Under the Curve (AUC) from prediction scores.\n",
    "    Args:\n",
    "        gt: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          true binary labels.\n",
    "        pred: Pytorch tensor on GPU, shape = [n_samples, n_classes]\n",
    "          can either be probability estimates of the positive class,\n",
    "          confidence values, or binary decisions.\n",
    "    Returns:\n",
    "        List of AUROCs of all classes.\n",
    "    \"\"\"\n",
    "    AUROCs = []\n",
    "    gt_np = gt.cpu().numpy()\n",
    "    pred_np = pred.cpu().numpy()\n",
    "    for i in range(N_CLASSES):\n",
    "        AUROCs.append(roc_auc_score(gt_np[:, i], pred_np[:, i]))\n",
    "    return AUROCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def main():\n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # initialize and load the model\n",
    "    device_ids = [0, 1, 2]\n",
    "    model = DenseNet121(N_CLASSES).cuda()\n",
    "    model = torch.nn.DataParallel(model , device_ids=device_ids).cuda()\n",
    "\n",
    "    if os.path.isfile(CKPT_PATH):\n",
    "        print(\"=> loading checkpoint\")\n",
    "        checkpoint = torch.load(CKPT_PATH)\n",
    "        model.load_state_dict(checkpoint['state_dict'] , strict=False)\n",
    "        print(\"=> loaded checkpoint\")\n",
    "    else:\n",
    "        print(\"=> no checkpoint found\")\n",
    "\n",
    "    normalize = transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                     [0.229, 0.224, 0.225])\n",
    "\n",
    "    test_dataset = DATASET(transform=transforms.Compose([\n",
    "                                        transforms.Resize(256),\n",
    "                                        transforms.TenCrop(224),\n",
    "                                        transforms.Lambda\n",
    "                                        (lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "                                        transforms.Lambda\n",
    "                                        (lambda crops: torch.stack([normalize(crop) for crop in crops]))\n",
    "                                    ]))\n",
    "    test_loader = DataLoader(dataset=test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=False, num_workers=8, pin_memory=True)\n",
    "\n",
    "    # initialize the ground truth and output tensor\n",
    "    gt = torch.FloatTensor()\n",
    "    gt = gt.cuda()\n",
    "    pred = torch.FloatTensor()\n",
    "    pred = pred.cuda()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    for i, (inp, target) in enumerate(test_loader):\n",
    "        target = target.cuda()\n",
    "        gt = torch.cat((gt, target), 0)\n",
    "        bs, n_crops, c, h, w = inp.size()\n",
    "        input_var = torch.autograd.Variable(inp.view(-1, c, h, w).cuda(), volatile=True)\n",
    "        output = model(input_var)\n",
    "        output_mean = output.view(bs, n_crops, -1).mean(1)\n",
    "        pred = torch.cat((pred, output_mean.data), 0)\n",
    "\n",
    "    AUROCs = compute_AUCs(gt, pred)\n",
    "    AUROC_avg = np.array(AUROCs).mean()\n",
    "    print('The average AUROC is {AUROC_avg:.3f}'.format(AUROC_avg=AUROC_avg))\n",
    "    for i in range(N_CLASSES):\n",
    "        print('The AUROC of {} is {}'.format(CLASS_NAMES[i], AUROCs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kmit-nvidia/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "/home/kmit-nvidia/.local/lib/python3.8/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=DenseNet121_Weights.IMAGENET1K_V1`. You can also use `weights=DenseNet121_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=> loading checkpoint\n",
      "=> loaded checkpoint\n",
      "Total number of Imgs 22433\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_906272/1635720245.py:45: UserWarning: volatile was removed and now has no effect. Use `with torch.no_grad():` instead.\n",
      "  input_var = torch.autograd.Variable(inp.view(-1, c, h, w).cuda(), volatile=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The average AUROC is 0.568\n",
      "The AUROC of Atelectasis is 0.5078725068374977\n",
      "The AUROC of Cardiomegaly is 0.5815592907352373\n",
      "The AUROC of Effusion is 0.5700464864857285\n",
      "The AUROC of Infiltration is 0.5726122360771466\n",
      "The AUROC of Mass is 0.5268263863853909\n",
      "The AUROC of Nodule is 0.5597481061271761\n",
      "The AUROC of Pneumonia is 0.5268196547554272\n",
      "The AUROC of Pneumothorax is 0.5709158377078678\n",
      "The AUROC of Consolidation is 0.6265536285261593\n",
      "The AUROC of Edema is 0.662728633225793\n",
      "The AUROC of Emphysema is 0.5208553104867718\n",
      "The AUROC of Fibrosis is 0.5778900765009758\n",
      "The AUROC of Pleural_Thickening is 0.5469677528805368\n",
      "The AUROC of Hernia is 0.6013906522816352\n"
     ]
    }
   ],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
